import requests
from bs4 import BeautifulSoup
import hashlib
import os
import re
import json
from datetime import datetime, timedelta

# =========================
# é…ç½®
# =========================
WEBHOOK_GENERAL = os.getenv("DISCORD_WEBHOOK_GENERAL")
WEBHOOK_CHEMICAL = os.getenv("DISCORD_WEBHOOK_CHEMICAL")
WEBHOOK_ENERGY = os.getenv("DISCORD_WEBHOOK_ENERGY")

SEEN_FILE = "seen_events.json"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# ç½å®³é—œéµå­—
FIRE = ["fire", "blaze", "ç«ç½", "ç«è­¦", "èµ·ç«", "å¤±ç«"]
EXPLOSION = ["explosion", "çˆ†ç‚¸", "æ°£çˆ†"]

# åˆ†æµé—œéµå­—
CHEMICAL = ["chemical", "petrochemical", "refinery", "çŸ³åŒ–", "åŒ–å·¥", "ç…‰æ²¹"]
ENERGY = ["power", "plant", "é›»å» ", "è®Šé›»æ‰€", "å„²èƒ½", "å¤ªé™½èƒ½", "é‹°é›»æ± "]
TECH = ["semiconductor", "electronics", "wafer", "åŠå°é«”", "é›»å­"]
BUILDING = ["building", "apartment", "skyscraper", "å¤§æ¨“", "ä½å®…"]

# æ’é™¤é›œè¨Šé—œéµå­— (å·²åŠ å…¥åŸ¹è¨“ã€æˆ¿å±‹ç­‰éäº‹æ•…è©å½™)
EXCLUDE = [
    "æ¼”ç·´", "æ¨¡æ“¬", "æ¼”ç¿’", "è¨“ç·´", "simulation", "drill", "exercise", "training",
    "è‚¡å¸‚", "æ”¿ç­–", "èª¿æŸ¥", "å§”å“¡æœƒ", "å ±å‘Š", "åŸå› ä»æœªç¢ºå®š", "èµ·ç«æˆå› ", "æ—¥å‰",
    "housing", "æˆ¿å±‹", "å®£å°", "å¹³å®‰ç¬¦", "é»ç‡ƒå¸‚å ´", "å£å…”å­", "é»ç‡ƒè˜‹æœ"
]

COUNTRY_MAP = {
    "greece": "ğŸ‡¬ğŸ‡·", "japan": "ğŸ‡¯ğŸ‡µ", "us": "ğŸ‡ºğŸ‡¸", "u.s.": "ğŸ‡ºğŸ‡¸", "america": "ğŸ‡ºğŸ‡¸",
    "uk": "ğŸ‡¬ğŸ‡§", "germany": "ğŸ‡©ğŸ‡ª", "china": "ğŸ‡¨ğŸ‡³", "taiwan": "ğŸ‡¹ğŸ‡¼"
}

# =========================
# å·¥å…·å‡½å¼
# =========================
def load_seen():
    if not os.path.exists(SEEN_FILE): return {}
    try:
        with open(SEEN_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return {}

def save_seen(data):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def translate_to_zh(text):
    try:
        res = requests.get("https://translate.googleapis.com/translate_a/single",
                           params={"client": "gtx", "sl": "auto", "tl": "zh-TW", "dt": "t", "q": text}, timeout=10)
        return res.json()[0][0][0]
    except: return text

def extract_event_core(title):
    """æå–äº‹æ•…æ ¸å¿ƒç‰¹å¾µä»¥ç”¢ç”Ÿå”¯ä¸€æŒ‡ç´‹"""
    t = title.lower()
    # åµæ¸¬ç½å®³èˆ‡è¨­æ–½ (åŠ å…¥åŒç¾©è©è½‰æ›)
    event_type = "fire" if any(k in t for k in FIRE) else "explosion"
    t = t.replace("cookie", "biscuit")
    facility_keywords = ["refinery", "biscuit", "factory", "plant", "semiconductor", "warehouse", "å·¥å» ", "å» æˆ¿", "é¤…ä¹¾", "ç…‰æ²¹å» "]
    facility = next((k for k in facility_keywords if k in t), "industrial_site")
    location = next((k for k in COUNTRY_MAP.keys() if k in t), "global")
    # ç§»é™¤æ•¸å­—(æ­»å‚·)èˆ‡é›œè¨Šå­—å…ƒ
    t_clean = re.sub(r"\d+", "", t)
    t_clean = re.sub(r"[^a-z\u4e00-\u9fff]", "", t_clean)
    return hashlib.sha256(f"{location}-{facility}-{event_type}".encode()).hexdigest()

def detect_country(text):
    t = text.lower()
    for k, flag in COUNTRY_MAP.items():
        if k in t: return flag
    return "ğŸŒ"

def classify_channel(title):
    t = title.lower()
    if any(k in t for k in CHEMICAL): return "CHEMICAL"
    if any(k in t for k in ENERGY): return "ENERGY"
    if any(k in t for k in TECH): return "TECH"
    if any(k in t for k in BUILDING): return "BUILDING"
    return "GENERAL"

def webhook_by_channel(ch):
    mapping = {"CHEMICAL": WEBHOOK_CHEMICAL, "ENERGY": WEBHOOK_ENERGY, "TECH": WEBHOOK_GENERAL, "BUILDING": WEBHOOK_GENERAL}
    return mapping.get(ch, WEBHOOK_GENERAL)

# =========================
# æ ¸å¿ƒåŸ·è¡Œ
# =========================
def run_realtime():
    seen_events = load_seen()
    feeds = [
        "https://news.google.com/rss/search?q=(factory+OR+industrial+OR+refinery)+(fire+OR+explosion)+when:12h&hl=en&gl=US&ceid=US:en",
        "https://news.google.com/rss/search?q=(å·¥å» +OR+å» æˆ¿+OR+é£Ÿå“å» )+(ç«ç½+OR+çˆ†ç‚¸)+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw",
    ]

    event_pool = {}
    now = datetime.now()

    for url in feeds:
        try:
            res = requests.get(url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(res.content, "xml")
            for item in soup.find_all("item")[:40]:
                title = item.title.text
                if any(k in title.lower() for k in EXCLUDE): continue
                if not any(k in title.lower() for k in FIRE + EXPLOSION): continue

                fp = extract_event_core(title)
                # è·¨æ¬¡åŸ·è¡Œå»é‡ï¼šå¦‚æœæª”æ¡ˆè£¡å·²ç¶“çœ‹éé€™å€‹äº‹ä»¶æŒ‡ç´‹ï¼Œç›´æ¥è·³é
                if fp in seen_events: continue

                if fp not in event_pool:
                    event_pool[fp] = {"titles": [title], "link": item.link.text, "pub": item.pubDate.text}
                else:
                    if title not in event_pool[fp]["titles"]:
                        event_pool[fp]["titles"].append(title)
        except Exception as e: print(f"RSS éŒ¯èª¤: {e}")

    sent_count = 0
    for fp, data in event_pool.items():
        main_title = sorted(data["titles"], key=len)[len(data["titles"])//2]
        flag = detect_country(main_title)
        channel = classify_channel(main_title)
        webhook = webhook_by_channel(channel)
        
        # æ¨™é¡Œç¿»è­¯ (éå°éä¸­æ‰ç¿»)
        is_chinese = bool(re.search(r"[\u4e00-\u9fff]", main_title))
        display_title = f"{main_title}\nï¼ˆ{translate_to_zh(main_title)}ï¼‰" if not is_chinese else main_title
        
        msg = (
            f"{flag} **å…¨çƒå·¥æ¥­äº‹æ•…é€šå ±**\n"
            f"ğŸ”¥ åˆ†é¡ï¼š`{channel}`\n"
            f"[{display_title}](<{data['link']}>)\n"
            f"ğŸ§  æ­¤äº‹ä»¶å·²æ•´åˆ `{len(data['titles'])}` å‰‡æ–°èä¾†æº\n"
            f"ğŸ•’ æ™‚é–“ï¼š`{data['pub']}`"
        )
        
        requests.post(webhook, json={"content": msg}, timeout=10)
        seen_events[fp] = now.isoformat()
        sent_count += 1

    # å¿ƒè·³æ©Ÿåˆ¶
    if sent_count == 0:
        requests.post(WEBHOOK_GENERAL, json={"content": "âœ… **ç³»çµ±ç›£æ¸¬æ­£å¸¸**\nç³»çµ±è¨­å®šçš„å‰ 12 å€‹å°æ™‚å…§ï¼Œç„¡æ–°å¢å·¥æ¥­äº‹æ•…æ–°èã€‚"})

    save_seen(seen_events)

if __name__ == "__main__":
    run_realtime()
