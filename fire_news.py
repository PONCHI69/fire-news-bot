import requests
from bs4 import BeautifulSoup
import os
import re

DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK")

# ç”¨ä¾†è¨˜éŒ„æœ¬è¼ªå·²ç™¼é€äº‹ä»¶çš„ã€Œç‰¹å¾µçµ„åˆã€
# æ ¼å¼ç‚º: "åœ°é»+äº‹ä»¶" (ä¾‹å¦‚: "å…§è’™å¤+çˆ†ç‚¸")
processed_events = set()

def translate_to_chinese(text):
    try:
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=zh-TW&dt=t&q={text}"
        res = requests.get(url)
        return res.json()[0][0][0]
    except:
        return text

def is_duplicate_event(title):
    """
    æ›´å¼·å¤§çš„å»é‡é‚è¼¯ï¼š
    1. æå–æ¨™é¡Œä¸­çš„ã€åœ°é»ã€(å¦‚: å…§è’™å¤, æ¡ƒåœ’, é›…å…¸)
    2. æå–æ¨™é¡Œä¸­çš„ã€æ ¸å¿ƒåè©ã€(å¦‚: é‹¼éµå» , å€‰åº«, å¡‘æ–™å» )
    3. å¦‚æœé€™å…©å€‹çµ„åˆåŒæ™‚å‡ºç¾éï¼Œå‰‡è¦–ç‚ºåŒä¸€äº‹ä»¶
    """
    # æå–åœ°å (2-4å€‹ä¸­æ–‡å­—)
    locations = re.findall(r'[\u4e00-\u9fa5]{2,4}', title)
    # æå–æ ¸å¿ƒåè©
    core_nouns = ["å·¥å» ", "å» æˆ¿", "å€‰åº«", "é‹¼éµ", "åŒ–å·¥", "çˆ†ç‚¸", "èµ·ç«", "å¡‘æ–™", "ç‰©æµ"]
    
    found_loc = ""
    found_noun = ""
    
    for loc in locations:
        if len(loc) >= 2:
            found_loc = loc
            break
            
    for noun in core_nouns:
        if noun in title:
            found_noun = noun
            break
            
    # å¦‚æœåŒæ™‚æ‰¾åˆ°åœ°é»èˆ‡æ ¸å¿ƒåè©ï¼Œå»ºç«‹ç‰¹å¾µå€¼
    if found_loc and found_noun:
        event_fingerprint = f"{found_loc}_{found_noun}"
        if event_fingerprint in processed_events:
            return True
        processed_events.add(event_fingerprint)
    
    # å‚™ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ¨™é¡Œè¶…é 70% ç›¸ä¼¼ï¼Œä¹Ÿè¦–ç‚ºé‡è¤‡ (é€™è£¡ç”¨ç°¡å–®çš„é•·åº¦æª¢æŸ¥)
    return False

def fetch_and_filter(rss_url, prefix, is_global=False):
    try:
        res = requests.get(rss_url)
        soup = BeautifulSoup(res.content, features="xml")
        
        # å·¥æ¥­ç™½åå–®
        industry_keywords = [
            "å» ", "å·¥æ¥­", "å·¥å» ", "åŒ–å·¥", "é‹¼éµ", "ç´¡ç¹”", "ç‰©æµ", "å€‰å„²", 
            "é›»å­å» ", "åŠå°é«”", "å» æˆ¿", "æ©Ÿå°", "å€‰åº«", "Warehouse", "Factory"
        ]
        
        # æ’é™¤é›œè¨Š
        exclude_keywords = [
            "ç¥¨æˆ¿", "é›»å½±", "è€åº—", "ç«é‹", "é¤å»³", "æ°‘å®…", "å…¬å¯“", "ä½å®…", 
            "æ©Ÿè»Š", "åœè»Šå ´", "é›œè‰", "å®¿èˆ", "å¸æ³•", "è­¦å‘Š", "å®£å°", "ç”³å ±", "ç•™æ‰"
        ]
        
        for item in soup.find_all('item')[:30]:
            title = item.title.text
            link = item.link.text
            
            # å…¨çƒæ–°èå…ˆç¿»è­¯ï¼Œæ–¹ä¾¿éæ¿¾
            display_title = translate_to_chinese(title) if is_global else title
            
            # 1. æª¢æŸ¥æ˜¯å¦åŒ…å«å·¥æ¥­é—œéµå­—
            is_industry = any(k.lower() in display_title.lower() for k in industry_keywords)
            # 2. æª¢æŸ¥æ˜¯å¦å«æœ‰æ’é™¤é—œéµå­—
            has_exclude = any(e.lower() in display_title.lower() for e in exclude_keywords)
            
            if is_industry and not has_exclude:
                # 3. æ¸…ç†ä¾†æºæ¨™ç±¤
                clean_title = re.sub(r'\s-\s.*$', '', display_title)
                
                # 4. æ ¸å¿ƒå»é‡æª¢æŸ¥
                if not is_duplicate_event(clean_title):
                    print(f"ç™¼é€æ–°äº‹ä»¶ï¼š{clean_title}")
                    send_to_discord(clean_title, link, prefix)
                else:
                    print(f"æ””æˆªé‡è¤‡äº‹ä»¶ï¼š{clean_title}")
                
    except Exception as e:
        print(f"å¤±æ•—: {e}")

def send_to_discord(title, link, prefix):
    payload = {"content": f"{prefix} ã€{title}ã€‘\nğŸ”— {link}"}
    try:
        requests.post(DISCORD_WEBHOOK_URL, json=payload)
    except:
        pass

if __name__ == "__main__":
    print("--- å•Ÿå‹• [å·¥æ¥­å»é‡ç‰ˆ] ç›£æ¸¬ç³»çµ± ---")
    tw_url = "https://news.google.com/rss/search?q=å·¥å» +OR+å» æˆ¿+OR+å·¥æ¥­å€+OR+çˆ†ç‚¸+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw"
    fetch_and_filter(tw_url, "ğŸ­ **å·¥æ¥­/å·¥å» ç«è­¦å ±å‘Š**")
    
    global_url = "https://news.google.com/rss/search?q=Factory+fire+OR+Industrial+explosion+OR+Warehouse+fire+when:12h&hl=en-US&gl=US&ceid=US:en"
    fetch_and_filter(global_url, "ğŸŒ **å…¨çƒå·¥æ¥­è­¦å ± (AIç¿»è­¯)**", is_global=True)
    print("--- ç›£æ¸¬çµæŸ ---")
