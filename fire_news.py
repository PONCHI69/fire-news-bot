import requests
from bs4 import BeautifulSoup
import hashlib
import os
from datetime import datetime, timedelta

# =========================
# åŸºæœ¬è¨­å®š
# =========================
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK")
SEEN_FILE = "seen_events.txt"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# æ“´å……åœ°é»èˆ‡äº‹æ•…è©å½™
FIRE_KEYWORDS = ["fire", "blaze", "ç«ç½", "ç«è­¦", "èµ·ç«", "ç‡’æ¯€", "æ•‘ç½", "é‹°é›»æ± ", "å¤ªé™½èƒ½", "å„²èƒ½", "å¤±ç«"]
EXPLOSION_KEYWORDS = ["explosion", "çˆ†ç‚¸", "æ°£çˆ†", "æ´©æ¼", "å™´å‡º"]
FACILITY_KEYWORDS = [
    "factory", "plant", "mill", "refinery", "warehouse", "å·¥å» ", "å» æˆ¿", "å€‰å„²", "å·¥æ¥­",
    "å…¬å¸", "ç§‘æŠ€", "é›»å­", "å» ", "å€‰åº«", "åœ’å€", "ä¸­å¿ƒ", "ä½œæ¥­", "ç¾å ´", "æ§½", "ç®¡", 
    "ä¸­æ²¹", "åŒ–å·¥", "æ²¹åº«", "é›»å» ", "å°å¡‘", "å›æ”¶", "çŸ³åŒ–", "ç…‰æ²¹", "åŒ–å­¸", "å¤§æ¨“", "è®Šé›»æ‰€", "æ—åœ’", "å¤§æ—"
]
EXCLUDE_KEYWORDS = [
    "æ¼”ç·´", "æ¨¡æ“¬", "æ¼”ç¿’", "å¯¦å…µ", "å®£å°", "è¨“ç·´", "simulation", "drill", "exercise",
    "éŠæˆ²", "steam", "æ¨¡æ“¬å™¨", "è‚¡å¸‚", "ç‡Ÿæ”¶", "è¬›åº§", "è«–å£‡", "ç ”è¨æœƒ", "æˆ¿å¸‚", "é é˜²",
    "é—–é—œ", "æ´»å‹•"
]

# =========================
# é‚è¼¯æ¨¡çµ„
# =========================
def event_key(title, link):
    return hashlib.sha256(f"{title}{link}".encode("utf-8")).hexdigest()

def is_duplicate(title, link):
    if not os.path.exists(SEEN_FILE): return False
    with open(SEEN_FILE, "r") as f:
        seen = f.read().splitlines()
    return event_key(title, link) in seen

def save_event(title, link):
    with open(SEEN_FILE, "a") as f:
        f.write(event_key(title, link) + "\n")

def check_match(title, is_global=False):
    t = title.lower()
    # å„ªå…ˆæ’é™¤é»‘åå–®
    if any(k in t for k in EXCLUDE_KEYWORDS): return False
    
    # åˆ¤æ–·æ˜¯å¦å«æœ‰ç«ç½/çˆ†ç‚¸å‹•ä½œ
    has_event = any(k in t for k in FIRE_KEYWORDS + EXPLOSION_KEYWORDS)
    
    if is_global:
        # åœ‹å¤–æ–°èæ”¾å¯¬ï¼šåªè¦æœ‰äº‹ä»¶å‹•è©ä¸”ä¸åœ¨é»‘åå–®å°±é€šé
        return has_event
    else:
        # åœ‹å…§æ–°èï¼šå¿…é ˆåŒæ™‚åŒ…å«äº‹ä»¶å‹•è©èˆ‡è¨­æ–½åœ°é»
        has_place = any(k in t for k in FACILITY_KEYWORDS)
        return has_event and has_place

def get_severity(title):
    t = title.lower()
    if any(k in t for k in ["dead", "killed", "fatal", "æ­»äº¡", "èº«äº¡"]): return "ğŸš¨ é‡å¤§å‚·äº¡"
    if any(k in t for k in ["injured", "å—å‚·"]): return "âš ï¸ æœ‰äººå—å‚·"
    if any(k in t for k in EXPLOSION_KEYWORDS): return "ğŸ’¥ ç™¼ç”Ÿçˆ†ç‚¸"
    return "ğŸ”¥ ç«è­¦é€šå ±"

def parse_time(date_str):
    try:
        gmt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %Z')
        tw = gmt + timedelta(hours=8)
        return tw.strftime('%Y-%m-%d %H:%M')
    except:
        return "æœªçŸ¥æ™‚é–“"

def translate_to_zh(text):
    try:
        res = requests.get("https://translate.googleapis.com/translate_a/single",
                           params={"client": "gtx", "sl": "en", "tl": "zh-TW", "dt": "t", "q": text}, timeout=10)
        return res.json()[0][0][0]
    except:
        return "ï¼ˆç¿»è­¯å¤±æ•—ï¼‰"

# =========================
# åŸ·è¡Œä¸»ç¨‹å¼
# =========================
def run_monitor():
    urls = [
        ("https://news.google.com/rss/search?q=(å·¥å» +OR+å» æˆ¿+OR+å·¥æ¥­å€+OR+åŒ–å·¥+OR+ç§‘æŠ€+OR+é›»å­+OR+å…¬å¸)+(ç«ç½+OR+çˆ†ç‚¸+OR+ç«è­¦+OR+èµ·ç«)+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw", "ğŸ­å·¥å» æƒ…å ±"),
        ("https://news.google.com/rss/search?q=(ä¸­æ²¹+OR+çŸ³åŒ–+OR+ç…‰æ²¹+OR+å°å¡‘+OR+æ—åœ’+OR+å¤§æ—)+(ç«ç½+OR+çˆ†ç‚¸+OR+èµ·ç«)+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw", "ğŸ”¥ç«è­¦"),
        ("https://news.google.com/rss/search?q=(factory+OR+industrial+OR+refinery)+(fire+OR+explosion)+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw", "ğŸŒå…¨çƒç«è­¦")
    ]

    for rss_url, prefix in urls:
        try:
            res = requests.get(rss_url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(res.content, features="xml")
            
            # å»ºç«‹åˆ†é¡æ——æ¨™ï¼Œç¢ºä¿æ¨™ç±¤æ–‡å­—èˆ‡ URLs å®šç¾©å®Œå…¨ä¸€è‡´
            is_global = (prefix == "ğŸŒå…¨çƒç«è­¦")
            
            for item in soup.find_all('item')[:20]:
                title = item.title.text
                link = item.link.text
                pub_date = item.pubDate.text if item.pubDate else ""
                tw_time = parse_time(pub_date)

                # å‚³å…¥åˆ†é¡æ——æ¨™æ±ºå®šéæ¿¾åš´æ ¼åº¦
                if check_match(title, is_global) and not is_duplicate(title, link):
                    # çµ„åˆè¨Šæ¯å‰å…ˆè¨ˆç®—åš´é‡ç¨‹åº¦
                    severity = get_severity(title)
                    display_title = title
                    
                    # åƒ…é‡å°å…¨çƒç«è­¦åŸ·è¡Œè‡ªå‹•ç¿»è­¯
                    if is_global:
                        translated = translate_to_zh(title)
                        display_title = f"{title}\nğŸ“ ç¿»è­¯: {translated}"
                    
                    # çµ„åˆæœ€çµ‚è¨Šæ¯æ ¼å¼
                    message = (
                        f"{prefix}\n"
                        f"**ã€{severity}ã€‘**\n"
                        f"[{display_title}](<{link}>)\n"
                        f"ğŸ•’ åŸå§‹ç™¼å¸ƒæ™‚é–“ (TW): `{tw_time}`"
                    )
                    
                    requests.post(DISCORD_WEBHOOK_URL, json={"content": message})
                    save_event(title, link)
        except Exception as e:
            print(f"éŒ¯èª¤: {e}")

if __name__ == "__main__":
    run_monitor()
