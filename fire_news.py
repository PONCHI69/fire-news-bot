import requests
from bs4 import BeautifulSoup
import os
import re

DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK")
sent_news_events = []

def translate_to_chinese(text):
    try:
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=zh-TW&dt=t&q={text}"
        res = requests.get(url)
        return res.json()[0][0][0]
    except:
        return text

def is_duplicate(title):
    # æå–æ¨™é¡Œä¸­çš„ä¸»è¦åœ°åæˆ–äº‹ä»¶åè©é€²è¡Œå»é‡
    keywords = re.findall(r'[\u4e00-\u9fa5]{2,4}', title)
    for word in keywords:
        if word in sent_news_events: return True
    for word in keywords:
        if len(word) >= 2: sent_news_events.append(word)
    return False

def fetch_and_filter(rss_url, prefix, is_global=False):
    try:
        res = requests.get(rss_url)
        soup = BeautifulSoup(res.content, features="xml")
        
        # 1. æŒ‡å®šå·¥æ¥­/å·¥å» ç›¸é—œé—œéµå­— (å¿…é ˆåŒ…å«å…¶ä¸­ä¹‹ä¸€)
        industry_keywords = [
            "å» ", "å·¥æ¥­", "å·¥å» ", "åŒ–å·¥", "é‹¼éµ", "ç´¡ç¹”", "é£Ÿå“å» ", "ç‰©æµ", 
            "å€‰å„²", "å»ºç¯‰", "å·¥åœ°", "é›»å­å» ", "åŠå°é«”", "é‹è¼¸", "ç…‰æ²¹", "åœ’å€",
            "æ©Ÿæˆ¿", "ä½œæ¥­å“¡", "å» æˆ¿", "æ©Ÿå°", "å•†å ´", "è³¼ç‰©ä¸­å¿ƒ", "Mall", 
            "Factory", "Plant", "Industrial", "Warehouse", "Construction"
        ]
        
        # 2. åš´æ ¼æ’é™¤æ¸…å–® (åŒ…å«é€™äº›ç›´æ¥åˆªé™¤)
        exclude_keywords = [
            "ç¥¨æˆ¿", "é›»å½±", "è€åº—", "ç«é‹", "é¤å»³", "æ°‘å®…", "å…¬å¯“", "ä½å®…", 
            "æ©Ÿè»Š", "åœè»Šå ´", "é›œè‰", "é‡è‰", "å®¿èˆ", "å¸æ³•", "è­¦å‘Š", "å®£å°",
            "æ½¤é¤…", "åº—é‹ª", "æ—…åº—", "è¡Œæ”¿", "æ¶ˆé˜²éšŠé•·", "å¸æ³•"
        ]
        
        for item in soup.find_all('item')[:25]:
            title = item.title.text
            link = item.link.text
            
            # å…ˆæª¢æŸ¥æ˜¯å¦æœ‰å·¥æ¥­/å¤§å‹å ´æ‰€é—œéµå­—
            is_industry = any(k.lower() in title.lower() for k in industry_keywords)
            # å†æª¢æŸ¥æ˜¯å¦å«æœ‰æ’é™¤é—œéµå­—
            has_exclude = any(e.lower() in title.lower() for e in exclude_keywords)
            
            # å…¨çƒæ–°èå…ˆç¿»è­¯å†åˆ¤æ–·ï¼Œæº–ç¢ºåº¦æ›´é«˜
            display_title = translate_to_chinese(title) if is_global else title
            
            # é‚è¼¯ï¼šå¿…é ˆæ˜¯å·¥æ¥­ç›¸é—œï¼Œä¸”ä¸èƒ½åœ¨æ’é™¤åå–®å…§
            if is_industry and not has_exclude:
                clean_title = re.sub(r'\s-\s.*$', '', display_title)
                if not is_duplicate(clean_title):
                    print(f"ç¬¦åˆå·¥æ¥­æ¨™æº–ï¼š{clean_title}")
                    send_to_discord(clean_title, link, prefix)
                
    except Exception as e:
        print(f"å¤±æ•—: {e}")

def send_to_discord(title, link, prefix):
    payload = {"content": f"{prefix} ã€{title}ã€‘\nğŸ”— {link}"}
    try:
        requests.post(DISCORD_WEBHOOK_URL, json=payload)
    except:
        pass

if __name__ == "__main__":
    print("--- å•Ÿå‹• [å·¥æ¥­ç´š] ç«ç½ç›£æ¸¬ç³»çµ± ---")
    tw_url = "https://news.google.com/rss/search?q=å·¥å» +OR+å» æˆ¿+OR+å·¥æ¥­å€+OR+çˆ†ç‚¸+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw"
    fetch_and_filter(tw_url, "ğŸ­ **å·¥æ¥­/å·¥å» ç«è­¦å ±å‘Š**")
    
    global_url = "https://news.google.com/rss/search?q=Factory+fire+OR+Industrial+explosion+OR+Warehouse+fire+when:12h&hl=en-US&gl=US&ceid=US:en"
    fetch_and_filter(global_url, "ğŸŒ **å…¨çƒå·¥æ¥­è­¦å ± (AIç¿»è­¯)**", is_global=True)
    print("--- ç›£æ¸¬çµæŸ ---")
