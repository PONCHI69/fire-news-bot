import requests
from bs4 import BeautifulSoup
import hashlib
import os
from datetime import datetime, timedelta

# =========================
# åŸºæœ¬è¨­å®šèˆ‡é—œéµå­—
# =========================
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK")
SEEN_FILE = "seen_events.txt"

FIRE_KEYWORDS = ["fire", "blaze", "ç«ç½", "ç«è­¦", "èµ·ç«", "ç‡’æ¯€", "æ•‘ç½", "é‹°é›»æ± ", "å¤ªé™½èƒ½", "å„²èƒ½", "å¤±ç«"]
EXPLOSION_KEYWORDS = ["explosion", "çˆ†ç‚¸", "æ°£çˆ†", "å™´å‡º", "æ´©æ¼"]
FACILITY_KEYWORDS = [
    "factory", "plant", "mill", "refinery", "warehouse", "å·¥å» ", "å» æˆ¿", "å€‰å„²", "å·¥æ¥­",
    "å…¬å¸", "ç§‘æŠ€", "é›»å­", "å» ", "å€‰åº«", "åœ’å€", "ä¸­å¿ƒ", "ä½œæ¥­", "ç¾å ´", "æ§½", "ç®¡", 
    "ä¸­æ²¹", "åŒ–å·¥", "æ²¹", "é›»å» ", "å°å¡‘", "å›æ”¶", "çŸ³åŒ–", "ç…‰æ²¹", "åŒ–å­¸", "å¤§æ¨“"
]
EXCLUDE_KEYWORDS = ["éŠæˆ²", "steam", "é™å…", "æ¨¡æ“¬å™¨", "å¤§äº¨", "ç¼ºå·¥", "é—œç¨…", "è‚¡å¸‚", "è¬›åº§", "è«–å£‡", "å…§é–£", "é¸", "é‡‘æ­£æ©", "ç ”è¨æœƒ", "ç‡Ÿæ”¶", "æˆ¿å¸‚"]

# =========================
# é‚è¼¯æ¨¡çµ„
# =========================
def is_duplicate(title, link):
    key = hashlib.sha256(f"{title}{link}".encode("utf-8")).hexdigest()
    if not os.path.exists(SEEN_FILE):
        return False
    with open(SEEN_FILE, "r") as f:
        seen = f.read().splitlines()
    return key in seen

def save_event(title, link):
    key = hashlib.sha256(f"{title}{link}".encode("utf-8")).hexdigest()
    with open(SEEN_FILE, "a") as f:
        f.write(key + "\n")

def check_match(title):
    t = title.lower()
    has_event = any(k in t for k in FIRE_KEYWORDS + EXPLOSION_KEYWORDS)
    has_place = any(k in t for k in FACILITY_KEYWORDS)
    has_exclude = any(k in t for k in EXCLUDE_KEYWORDS)
    return has_event and has_place and not has_exclude

def get_severity(title):
    if any(k in title for k in ["æ­»", "killed", "dead", "fatal"]): return "ğŸš¨ é‡å¤§å‚·äº¡"
    if any(k in title for k in ["å‚·", "injured"]): return "âš ï¸ æœ‰äººå—å‚·"
    if any(k in title for k in EXPLOSION_KEYWORDS): return "ğŸ’¥ ç™¼ç”Ÿçˆ†ç‚¸"
    return "ğŸ”¥ ç«è­¦é€šå ±"

def parse_time(date_str):
    # å°‡ RSS çš„ GMT æ™‚é–“è½‰æ›ç‚ºå°ç£æ™‚é–“ (UTC+8)
    try:
        # æ ¼å¼ç¯„ä¾‹: Fri, 23 Jan 2026 15:00:00 GMT
        gmt_time = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %Z')
        tw_time = gmt_time + timedelta(hours=8)
        return tw_time.strftime('%Y-%m-%d %H:%M')
    except:
        return "æœªçŸ¥æ™‚é–“"

# =========================
# åŸ·è¡Œä¸»ç¨‹å¼
# =========================
def run_monitor():
    urls = [
        ("https://news.google.com/rss/search?q=(å·¥å» +OR+å» æˆ¿+OR+çŸ³åŒ–+OR+å·¥æ¥­å€+OR+åŒ–å·¥+OR+å» +OR+ç§‘æŠ€+OR+é›»å­+OR+ä¸­æ²¹)+(ç«ç½+OR+çˆ†ç‚¸+OR+ç«è­¦+OR+èµ·ç«)+when:24h&hl=zh-TW&gl=TW&ceid=TW:zh-tw", "ğŸ­ å·¥æ¥­/å·¥å» æƒ…å ±"),
        ("https://news.google.com/rss/search?q=(factory+OR+industrial+OR+refinery)+(fire+OR+explosion)+when:24h&hl=zh-TW&gl=TW&ceid=TW:zh-tw", "ğŸŒ å…¨çƒå·¥æ¥­è­¦å ±")
    ]

    for rss_url, prefix in urls:
        try:
            res = requests.get(rss_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
            soup = BeautifulSoup(res.content, features="xml")
            items = soup.find_all('item')
            
            for item in items[:15]:
                title = item.title.text
                link = item.link.text
                pub_date = item.pubDate.text if item.pubDate else ""
                tw_time_str = parse_time(pub_date)
                
                if check_match(title) and not is_duplicate(title, link):
                    severity = get_severity(title)
                    # çµ„åˆè¨Šæ¯ï¼šåŠ å…¥æ™‚é–“æˆ³è¨˜ (ä½¿ç”¨ Discord çš„ç¨‹å¼ç¢¼å€å¡Šèªæ³•è®“æ™‚é–“æ›´é¡¯çœ¼)
                   message = (
                       f"{prefix}\n"
                       f"**ã€{severity}ã€‘**\n"
                       f"[{title}](<{link}>)\n"
                       f"ğŸ•’ åŸå§‹ç™¼å¸ƒæ™‚é–“ (TW): `{tw_time_str}`"
                    )
