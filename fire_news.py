import requests
from bs4 import BeautifulSoup
import hashlib
import os
from typing import List

# ========================
# è¨­å®šå€
# ========================
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK")
SEEN_FILE = "seen_events.txt"

# ========================
# äº‹ä»¶å»é‡æ¨¡çµ„ (è¨˜éŒ„å·²ç™¼é€éçš„æ–°è)
# ========================
class EventDeduplicator:
    def __init__(self, filepath: str):
        self.filepath = filepath
        self.seen = self._load_seen()

    def _load_seen(self) -> set:
        if not os.path.exists(self.filepath):
            return set()
        with open(self.filepath, "r") as f:
            return set(line.strip() for line in f)

    def is_duplicate(self, title: str, link: str) -> bool:
        # å»ºç«‹å”¯ä¸€æŒ‡ç´‹
        key = hashlib.sha256(f"{title}{link}".encode("utf-8")).hexdigest()
        if key in self.seen:
            return True
        self.seen.add(key)
        with open(self.filepath, "a") as f:
            f.write(key + "\n")
        return False

# ========================
# é—œéµå­—æ¯”å°æ¨¡çµ„ (åš´æ ¼éæ¿¾é‚è¼¯)
# ========================
class KeywordMatcher:
    def __init__(self, fire_keywords, place_keywords, exclude_keywords):
        self.fire_keywords = fire_keywords
        self.place_keywords = place_keywords
        self.exclude_keywords = exclude_keywords

    def match(self, text: str) -> bool:
        t = text.lower()
        # 1. å¿…é ˆå«æœ‰ç«ç½å‹•è©
        has_fire = any(k in t for k in self.fire_keywords)
        # 2. å¿…é ˆå«æœ‰å·¥æ¥­åœ°é»
        has_place = any(k in t for k in self.place_keywords)
        # 3. çµ•å°ä¸èƒ½å«æœ‰é»‘åå–® (è§£æ±ºéŠæˆ²ã€è¬›åº§é›œè¨Š)
        has_exclude = any(e in t for e in self.exclude_keywords)
        return has_fire and has_place and not has_exclude

# ========================
# ä¸»ç¨‹å¼
# ========================
def run_monitor():
    dedup = EventDeduplicator(SEEN_FILE)
    matcher = KeywordMatcher(
        fire_keywords=["ç«ç½", "ç«è­¦", "çˆ†ç‚¸", "æ°£çˆ†", "èµ·ç«", "ç‡’æ¯€", "æ•‘ç½", "fire", "explosion"],
        place_keywords=["å» ", "å·¥æ¥­", "å€‰åº«", "åœ’å€", "å» æˆ¿", "å€‰å„²", "factory", "warehouse"],
        exclude_keywords=["éŠæˆ²", "steam", "é™å…", "å¤§äº¨", "æ¨¡æ“¬å™¨", "ç¼ºå·¥", "é—œç¨…", "è‚¡å¸‚", "æ‹›å‹Ÿ", "è¬›åº§", "è«–å£‡", "ç ”è¨æœƒ", "æ³•èªª"]
    )

    urls = [
        ("https://news.google.com/rss/search?q=\"å·¥å» \"+(ç«ç½+OR+çˆ†ç‚¸+OR+ç«è­¦)+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw", "ğŸ­ **å·¥æ¥­/å·¥å» ç«è­¦å ±å‘Š**"),
        ("https://news.google.com/rss/search?q=(\"factory\"+OR+\"industrial\")+(fire+OR+explosion)+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw", "ğŸŒ **å…¨çƒå·¥æ¥­è­¦å ± (AIç¿»è­¯)**")
    ]

    for rss_url, prefix in urls:
        try:
            res = requests.get(rss_url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(res.content, features="xml")
            for item in soup.find_all('item')[:15]:
                title = item.title.text
                link = item.link.text
                
                # éæ¿¾ä¸¦å»é‡
                if matcher.match(title) and not dedup.is_duplicate(title, link):
                    print(f"ğŸš€ ç™¼é€æ–°äº‹ä»¶: {title}")
                    payload = {"content": f"{prefix}\n**{title}**\nğŸ”— {link}"}
                    requests.post(DISCORD_WEBHOOK_URL, json=payload)
        except Exception as e:
            print(f"æŠ“å–å¤±æ•—: {e}")

if __name__ == "__main__":
    run_monitor()
