import requests
from bs4 import BeautifulSoup
import os
import re

DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK")

# ç”¨ä¾†è¨˜éŒ„æœ¬è¼ªå·²ç¶“ç™¼é€éçš„æ ¸å¿ƒé—œéµå­—ï¼Œé˜²æ­¢é‡è¤‡
sent_news_events = []

def translate_to_chinese(text):
    """ç°¡å–®çš„ Google ç¿»è­¯ API"""
    try:
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=zh-TW&dt=t&q={text}"
        res = requests.get(url)
        return res.json()[0][0][0]
    except:
        return text

def is_duplicate(title):
    """
    ç°¡å–®çš„å»é‡é‚è¼¯ï¼šæª¢æŸ¥æ¨™é¡Œä¸­æ˜¯å¦åŒ…å«å·²ç™¼é€éçš„é—œéµåœ°é»æˆ–åè©ã€‚
    ä¾‹å¦‚ï¼šæ¨™é¡Œæœ‰ã€Œå¡æ‹‰å¥‡ã€ä¸”ä¹‹å‰ç™¼éã€Œå¡æ‹‰å¥‡ã€ï¼Œå°±è¦–ç‚ºé‡è¤‡ã€‚
    """
    # æå–æ¨™é¡Œä¸­çš„ä¸»è¦åœ°åæˆ–åè©ï¼ˆç°¡å–®éæ¿¾ 2-4 å€‹å­—çš„åè©ï¼‰
    # é€™æ˜¯ä¸€å€‹åŸºç¤é‚è¼¯ï¼Œå¯ä»¥æ ¹æ“šéœ€æ±‚èª¿æ•´
    keywords = re.findall(r'[\u4e00-\u9fa5]{2,4}', title)
    
    for word in keywords:
        if word in sent_news_events:
            return True
    
    # å¦‚æœæ˜¯å…¨æ–°çš„æ–°èï¼Œå°‡ä¸»è¦è©å½™å­˜å…¥ç´€éŒ„
    for word in keywords:
        if len(word) >= 2:
            sent_news_events.append(word)
    return False

def send_to_discord(title, link, prefix):
    """å°‡è¨Šæ¯ç™¼é€è‡³ Discord"""
    # åœ¨ç™¼é€å‰æª¢æŸ¥æ˜¯å¦é‡è¤‡
    if is_duplicate(title):
        print(f"è·³éé‡è¤‡æ–°èï¼š{title}")
        return

    payload = {"content": f"{prefix} ã€{title}ã€‘\nğŸ”— {link}"}
    try:
        requests.post(DISCORD_WEBHOOK_URL, json=payload)
    except:
        pass

def fetch_and_filter(rss_url, prefix, is_global=False):
    """æŠ“å–ä¸¦é€²è¡Œåš´æ ¼æª¢æŸ¥èˆ‡ç¿»è­¯"""
    try:
        res = requests.get(rss_url)
        soup = BeautifulSoup(res.content, features="xml")
        
        valid_keywords = ["ç«", "çˆ†ç‚¸", "æ°£çˆ†", "ç«è­¦", "ç„šæ¯€", "Fire", "Explosion", "Blast"]
        exclude_keywords = [
            "è­¦å‘Š", "å®£å°", "æé†’", "å‘¼ç±²", "å¸æ³•", "ç¨‹åº", "å¾‹å¸«", "ç½æ°‘", "å–„å¾Œ",
            "å®¶", "å±‹", "House", "Garage", "Home", "Apartment", "Residential",
            "è²·æ°£", "æ•ˆèƒ½", "è‚¡å¸‚", "å€‹è³‡", "ç§˜å¯†", "åå–®", "ç†±åº¦", "é›œè‰", "é‡è‰"
        ]
        
        for item in soup.find_all('item')[:20]: # å¢åŠ æƒææ•¸é‡ç¢ºä¿ä¸æ¼æ‰
            title = item.title.text
            link = item.link.text
            
            has_valid = any(k.lower() in title.lower() for k in valid_keywords)
            has_exclude = any(e.lower() in title.lower() for e in exclude_keywords)
            
            if has_valid and not has_exclude:
                display_title = translate_to_chinese(title) if is_global else title
                # æ¸…é™¤æ¨™é¡Œæœ«å°¾çš„ä¾†æºæ¨™ç±¤ï¼ˆä¾‹å¦‚ - è‡ªç”±æ™‚å ±ï¼‰ï¼Œå¢åŠ å»é‡æº–ç¢ºåº¦
                clean_title = re.sub(r'\s-\s.*$', '', display_title)
                
                print(f"å˜—è©¦ç™¼é€ï¼š{clean_title}")
                send_to_discord(clean_title, link, prefix)
                
    except Exception as e:
        print(f"å¤±æ•—: {e}")

if __name__ == "__main__":
    print("--- å•Ÿå‹•å»é‡ç¿»è­¯ç›£æ¸¬ç³»çµ± ---")
    
    # 1. å°ç£èˆ‡å…©å²¸æœå°‹ (12å°æ™‚å…§)
    tw_url = "https://news.google.com/rss/search?q=ç«ç½+OR+çˆ†ç‚¸+OR+ç«è­¦+when:12h&hl=zh-TW&gl=TW&ceid=TW:zh-tw"
    fetch_and_filter(tw_url, "ğŸ‡¹ğŸ‡¼ **å°ç£/å…©å²¸å³æ™‚ç«è­¦**")
    
    # 2. å…¨çƒè‹±æ–‡æœå°‹ (12å°æ™‚å…§)
    global_url = "https://news.google.com/rss/search?q=Fire+OR+Explosion+when:12h&hl=en-US&gl=US&ceid=US:en"
    fetch_and_filter(global_url, "ğŸŒ **å…¨çƒé‡å¤§è­¦å ± (è‡ªå‹•ç¿»è­¯)**", is_global=True)
    
    print("--- ç›£æ¸¬çµæŸ ---")
